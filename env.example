# ===========================================
# AI-Talk Backend Configuration
# ===========================================

# Server Configuration
HOST=0.0.0.0
PORT=8000
DEBUG=false
ENVIRONMENT=development

# ===========================================
# Model Configuration
# ===========================================

# LLM Model (Qwen2.5-7B-Instruct AWQ 4-bit quantized)
# VRAM Usage: ~4-5 GB
# Docs: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-AWQ
LLM_MODEL_NAME=Qwen/Qwen2.5-7B-Instruct-AWQ
LLM_MAX_MODEL_LEN=4096
# Reduced to 0.35 to leave VRAM for STT (~6-8GB) and TTS (~4-5GB)
# Total estimated VRAM: ~16-20GB (fits on 24GB GPU)
LLM_GPU_MEMORY_UTILIZATION=0.35

# ASR/STT Model (Kyutai STT 2.6B English)
# VRAM Usage: ~6-8 GB
# Latency: 2.5 second delay (streaming)
# Docs: https://huggingface.co/kyutai/stt-2.6b-en
ASR_MODEL_NAME=kyutai/stt-2.6b-en
ASR_SAMPLE_RATE=16000
# Prefer Moshi backend for Kyutai STT (recommended for stability)
ASR_PREFER_MOSHI=true
# Keep false unless you explicitly want to test Kyutai transformers backend
ASR_ENABLE_KYUTAI_TRANSFORMERS=false
# Attention implementation when Kyutai transformers is enabled: eager|sdpa|flash_attention_2
ASR_KYUTAI_ATTN_IMPLEMENTATION=eager

# TTS Model (Kyutai TTS 1.8B English/French)
# VRAM Usage: ~4-5 GB
# Supports: English and French
# Docs: https://huggingface.co/kyutai/tts-1.6b-en_fr
TTS_MODEL_NAME=kyutai/tts-1.6b-en_fr
TTS_SAMPLE_RATE=24000

# ===========================================
# Model Loading Options
# ===========================================

# Set to true to load models on startup (recommended for production)
# Set to false for faster startup during development
PRELOAD_MODELS=true

# Local path to store downloaded models
# Windows example: C:/models/ai-talk
# Linux/Docker example: /app/models
MODELS_CACHE_DIR=/app/models

# ===========================================
# GPU Configuration
# ===========================================

# Force specific CUDA device (useful for multi-GPU systems)
# Leave empty for auto-detection
CUDA_VISIBLE_DEVICES=0

# Enable TensorFloat-32 for faster computation on Ampere+ GPUs
TORCH_TF32=1

# ===========================================
# WebSocket Configuration
# ===========================================

# Audio chunk size in milliseconds
AUDIO_CHUNK_MS=100

# Maximum conversation duration in seconds (0 = unlimited)
MAX_CONVERSATION_DURATION=1800

# WebSocket ping interval in seconds
WS_PING_INTERVAL=30

# ===========================================
# Redis Configuration (Optional)
# ===========================================

# Set to true to enable Redis for session management
REDIS_ENABLED=false
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_DB=0

# ===========================================
# CORS Configuration
# ===========================================

# Comma-separated list of allowed origins
CORS_ORIGINS=http://localhost:3000,https://your-frontend-domain.com

# ===========================================
# Logging
# ===========================================

LOG_LEVEL=INFO

# ===========================================
# Hardware Requirements Reference
# ===========================================
# Minimum GPU VRAM: 24 GB (RTX 4090, RTX 3090, A6000)
# Minimum System RAM: 32 GB
# Minimum Storage: 50 GB SSD (for models)
# CUDA Version: 12.1+
# Python Version: 3.11+
